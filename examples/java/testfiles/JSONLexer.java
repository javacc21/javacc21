/* Generated by: JavaCC 21 Parser Generator. JSONLexer.java */
package org.parsers.json;

import static org.parsers.json.JSONConstants.TokenType.*;
import java.io.*;
import java.util.logging.Logger;
import java.util.*;
public class JSONLexer implements JSONConstants {
    private String inputSource= "input";
    // The following two BitSets are used to store 
    // the current active NFA states in the core tokenization loop
    private BitSet nextStates= new BitSet(), currentStates= new BitSet();
    // Holder for the pending characters we read from the input stream
    private final StringBuilder charBuff= new StringBuilder();
    EnumSet<TokenType> activeTokenTypes= EnumSet.allOf(TokenType.class);
    // Just used to "bookmark" the starting location for a token
    // for when we put in the location info at the end.
    private int tokenBeginLine, tokenBeginColumn;
    // Token types that are "regular" tokens that participate in parsing,
    // i.e. declared as TOKEN
    static final EnumSet<TokenType> regularTokens= EnumSet.of(TokenType.EOF, TokenType.COLON, TokenType.COMMA, TokenType.OPEN_BRACKET, TokenType.CLOSE_BRACKET, TokenType.OPEN_BRACE, TokenType.CLOSE_BRACE, TokenType.TRUE, TokenType.FALSE, TokenType.NULL, TokenType.STRING_LITERAL, TokenType.NUMBER);
    // Token types that do not participate in parsing, a.k.a. "special" tokens in legacy JavaCC,
    // i.e. declared as UNPARSED (or SPECIAL_TOKEN)
    static private final EnumSet<TokenType> unparsedTokens= EnumSet.noneOf(TokenType.class);
    // Tokens that are skipped, i.e. SKIP
    static final EnumSet<TokenType> skippedTokens= EnumSet.of(TokenType.WHITESPACE);
    // Tokens that correspond to a MORE, i.e. that are pending 
    // additional input
    static private final EnumSet<TokenType> moreTokens= EnumSet.noneOf(TokenType.class);
    private static final Logger LOGGER= Logger.getLogger(JSONConstants.class.getName());
    private boolean trace_enabled= false;
    private InvalidToken invalidToken;
    private Token previousToken;
    // The source of the raw characters that we are scanning  
    FileLineMap input_stream;
    public String getInputSource() {
        return inputSource;
    }

    public void setInputSource(String inputSource) {
        this.inputSource= inputSource;
        input_stream.setInputSource(inputSource);
    }

    public JSONLexer(CharSequence chars) {
        this("input", chars);
    }

    public JSONLexer(String inputSource, CharSequence chars) {
        this(inputSource, chars, LexicalState.JSON, 1, 1);
    }

    public JSONLexer(String inputSource, CharSequence chars, LexicalState lexState, int line, int column) {
        this.inputSource= inputSource;
        input_stream= new FileLineMap(inputSource, chars, line, column);
        switchTo(lexState);
    }

    public JSONLexer(Reader reader) {
        this("input", reader, LexicalState.JSON, 1, 1);
    }

    public JSONLexer(String inputSource, Reader reader) {
        this(inputSource, reader, LexicalState.JSON, 1, 1);
    }

    public JSONLexer(String inputSource, Reader reader, LexicalState lexState, int line, int column) {
        this.inputSource= inputSource;
        input_stream= new FileLineMap(inputSource, reader, line, column);
        switchTo(lexState);
    }

    /**
   * The public method for getting the next token.
   * Most of the work is done in the private method
   * nextToken, which invokes the NFA machinery
   */
    public Token getNextToken() {
        Token token= null;
        do {
            token= nextToken();
        }
        while (token instanceof InvalidToken);
        if (invalidToken!=null) {
            invalidToken.setNextToken(token);
            token.setPreviousToken(invalidToken);
            Token it= invalidToken;
            this.invalidToken= null;
            return it;
        }
        token.setPreviousToken(previousToken);
        if (previousToken!=null) previousToken.setNextToken(token);
        return previousToken= token;
    }

    // The main method to invoke the NFA machinery
    private final Token nextToken() {
        Token matchedToken= null;
        boolean inMore= false;
        int matchedPos, charsRead, curChar;
        // The core tokenization loop
        while (matchedToken== null) {
            TokenType matchedType= null;
            matchedPos= charsRead= 0;
            if (inMore) {
                curChar= input_stream.readChar();
                if (curChar>=0) charBuff.appendCodePoint(curChar);
            }
            else  {
                charBuff.setLength(0);
                tokenBeginLine= input_stream.getLine();
                tokenBeginColumn= input_stream.getColumn();
                curChar= input_stream.readChar();
                if (trace_enabled) LOGGER.info("Starting new token on line: "+tokenBeginLine+", column: "+tokenBeginColumn);
                if (curChar== -1) {
                    if (trace_enabled) LOGGER.info("Reached end of input");
                    matchedType= TokenType.EOF;
                }
                else  {
                    if (trace_enabled) LOGGER.info("Read character "+JSONConstants.displayChar(curChar));
                    charBuff.appendCodePoint(curChar);
                }
            }
            JSONNfaData.NfaFunction[] nfaFunctions= JSONNfaData.getFunctionTableMap(lexicalState);
            // the core NFA loop
            if (matchedType!=TokenType.EOF) do {
                // Holder for the new type (if any) matched on this iteration
                TokenType newType= null;
                if (charsRead> 0) {
                    // What was nextStates on the last iteration 
                    // is now the currentStates!
                    BitSet temp= currentStates;
                    currentStates= nextStates;
                    nextStates= temp;
                    int retval= input_stream.readChar();
                    if (trace_enabled) LOGGER.info("Read character "+JSONConstants.displayChar(retval));
                    if (retval>=0) {
                        curChar= retval;
                        charBuff.appendCodePoint(curChar);
                    }
                    else break;
                }
                nextStates.clear();
                if (charsRead== 0) {
                    newType= nfaFunctions[0].apply(curChar, nextStates);
                    if (!activeTokenTypes.contains(newType)) newType= null;
                }
                else  {
                    int nextActive= currentStates.nextSetBit(0);
                    while (nextActive!=-1) {
                        TokenType returnedType= nfaFunctions[nextActive].apply(curChar, nextStates);
                        if (activeTokenTypes.contains(returnedType)&&(newType== null||returnedType.ordinal()<newType.ordinal())) {
                            newType= returnedType;
                            if (trace_enabled) LOGGER.info("Potential match: "+newType);
                        }
                        nextActive= currentStates.nextSetBit(nextActive+1);
                    }
                }
                ++charsRead;
                if (newType!=null) {
                    matchedType= newType;
                    inMore= moreTokens.contains(matchedType);
                    matchedPos= charsRead;
                }
            }
            while (!nextStates.isEmpty());
            if (matchedType== null) {
                backup(charsRead-1);
                if (trace_enabled) LOGGER.info("Invalid input: "+JSONConstants.displayChar(charBuff.codePointAt(0)));
                return handleInvalidChar(charBuff.codePointAt(0));
            }
            else  {
                if (trace_enabled) LOGGER.info("Matched pattern of type: "+matchedType+": "+JSONConstants.addEscapes(charBuff.toString()));
            }
            if (charsRead> matchedPos) backup(charsRead-matchedPos);
            if (regularTokens.contains(matchedType)||unparsedTokens.contains(matchedType)) {
                matchedToken= instantiateToken(matchedType);
            }
        }
        return matchedToken;
    }

    public final void backup(int amount) {
        input_stream.backup(amount);
        truncateCharBuff(charBuff, amount);
    }

    /**
   * Truncate a StringBuilder by a certain number of code points
   * @param buf the StringBuilder
   * @param amount the number of code points to truncate
   */
    static final void truncateCharBuff(StringBuilder buf, int amount) {
        int idx= buf.length();
        if (idx<=amount) idx= 0;
        while (idx> 0&&amount--> 0) {
            char ch= buf.charAt(--idx);
            if (Character.isLowSurrogate(ch))--idx;
        }
        buf.setLength(idx);
    }

    LexicalState lexicalState= LexicalState.values()[0];
    /** 
     * Switch to specified lexical state. 
     * @param lexState the lexical state to switch to
     * @return whether we switched (i.e. we weren't already in the desired lexical state)
     */
    public boolean switchTo(LexicalState lexState) {
        if (this.lexicalState!=lexState) {
            if (trace_enabled) LOGGER.info("Switching from lexical state "+this.lexicalState+" to "+lexState);
            this.lexicalState= lexState;
            return true;
        }
        return false;
    }

    // Reset the token source input
    // to just after the Token passed in.
    void reset(Token t, LexicalState state) {
        input_stream.goTo(t.getEndLine(), t.getEndColumn());
        input_stream.forward(1);
        t.setNext(null);
        t.setNextToken(null);
        if (state!=null) {
            switchTo(state);
        }
    }

    void reset(Token t) {
        reset(t, null);
    }

    FileLineMap getFileLineMap() {
        return input_stream;
    }

    private InvalidToken handleInvalidChar(int ch) {
        int line= input_stream.getEndLine();
        int column= input_stream.getEndColumn();
        String img= new String(new int[]{ch}, 0, 1);
        if (invalidToken== null) {
            invalidToken= new InvalidToken(img, inputSource);
            invalidToken.setBeginLine(line);
            invalidToken.setBeginColumn(column);
        }
        else  {
            invalidToken.setImage(invalidToken.getImage()+img);
        }
        invalidToken.setEndLine(line);
        invalidToken.setEndColumn(column);
        return invalidToken;
    }

    private Token instantiateToken(TokenType type) {
        String tokenImage= charBuff.toString();
        Token matchedToken= Token.newToken(type, tokenImage, this);
        matchedToken.setBeginLine(tokenBeginLine);
        matchedToken.setEndLine(input_stream.getEndLine());
        matchedToken.setBeginColumn(tokenBeginColumn);
        matchedToken.setEndColumn(input_stream.getEndColumn());
        matchedToken.setInputSource(this.inputSource);
        matchedToken.setUnparsed(unparsedTokens.contains(type));
        return matchedToken;
    }

    // Generate the map for lexical state transitions from the various token types (if necessary)
}
