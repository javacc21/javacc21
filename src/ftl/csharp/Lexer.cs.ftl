[#ftl strict_vars=true]
[#--
  Copyright (C) 2008-2020 Jonathan Revusky, revusky@javacc.com
  Copyright (C) 2021-2022 Vinay Sajip, vinay_sajip@yahoo.co.uk
  All rights reserved.

  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions are met:

      * Redistributions of source code must retain the above copyright
        notices, this list of conditions and the following disclaimer.
      * Redistributions in binary form must reproduce the above copyright
        notice, this list of conditions and the following disclaimer in
        the documentation and/or other materials provided with the
        distribution.
      * None of the names Jonathan Revusky, Vinay Sajip, Sun
        Microsystems, Inc. nor the names of any contributors may be
        used to endorse or promote products derived from this software
        without specific prior written permission.

  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
  THE POSSIBILITY OF SUCH DAMAGE.
--]
// Generated by ${generated_by}. Do not edit.
// ReSharper disable InconsistentNaming
[#import "CommonUtils.inc.ftl" as CU]
[#var NFA_RANGE_THRESHOLD = 16]
[#var MAX_INT=2147483647]
[#var lexerData=grammar.lexerData]
[#var multipleLexicalStates = lexerData.lexicalStates.size() > 1]
[#var TT = "TokenType."]
[#var injector = grammar.injector]
[#var PRESERVE_LINE_ENDINGS=grammar.preserveLineEndings?string("true", "false")
      JAVA_UNICODE_ESCAPE= grammar.javaUnicodeEscape?string("true", "false")
      ENSURE_FINAL_EOL = grammar.ensureFinalEOL?string("true", "false")
      PRESERVE_TABS = grammar.preserveTabs?string("true", "false")
 ]

[#macro EnumSet varName tokenNames indent=0]
[#var is = ""?right_pad(indent)]
[#if tokenNames?size=0]
${is}private static HashSet<TokenType> ${varName} = Utils.GetOrMakeSet();
[#else]
${is}private static HashSet<TokenType> ${varName} = Utils.GetOrMakeSet(
[#list tokenNames as type]
${is}    TokenType.${type}[#if type_index < (tokenNames?size - 1)],[/#if]
[/#list]
${is});
[/#if]
[/#macro]

[#--
  Generate all the NFA transition code
  for the given lexical state
--]
[#macro GenerateStateCode lexicalState]
  [#list lexicalState.allNfaStates as nfaState]
    [#if nfaState.moveRanges.size() >= NFA_RANGE_THRESHOLD]
      [@GenerateMoveArray nfaState /]
    [/#if]
    [#if nfaState.composite]
       [@GenerateCompositeNfaMethod nfaState /]
    [#else]
       [@GenerateSimpleNfaMethod nfaState /]
    [/#if]
[/#list]

        private static void NFA_FUNCTIONS_${lexicalState.name}Init() {
            var f = new NfaFunction[] {
[#list lexicalState.allNfaStates as state]
                ${state.methodName}[#if state_has_next],[/#if]
[/#list]
            };
    [#if multipleLexicalStates]
            functionTableMap.Add(LexicalState.${lexicalState.name}, f);
    [#else]
            nfaFunctions = f;
    [/#if]
        }

[/#macro]

[#--
   Generate the array representing the characters
   that this NfaState "accepts".
   This corresponds to the moveRanges field in
   com.javacc.core.NfaState
--]
[#macro GenerateMoveArray nfaState]
[#var moveRanges = nfaState.moveRanges]
[#var arrayName = nfaState.movesArrayName]
        private static int[] ${arrayName} = {
[#list nfaState.moveRanges as char]
            ${grammar.utils.displayChar(char)}[#if char_has_next],[/#if]
[/#list]
        };

[/#macro]

[#--
   Generate the method that represents the transitions
   that correspond to an instanceof com.javacc.core.CompositeNfaState
--]
[#macro GenerateCompositeNfaMethod nfaState]
        static TokenType? ${nfaState.methodName}(int ch, BitSet nextStates, HashSet<TokenType> validTypes) {
            TokenType? type = null;
    [#var states = nfaState.orderedStates, lastBlockStartIndex=0, useIf=false]
    [#list states as state]
      [#if state_index ==0 || !state.moveRanges.equals(states[state_index-1].moveRanges)]
          [#-- In this case we need a new if or possibly else if --]
         [#if state_index == 0 || state.overlaps(states.subList(lastBlockStartIndex, state_index))]
           [#-- If there is overlap between this state and any of the states
                 handled since the last lone if, we start a new if-else
                 If not, we continue in the same if-else block as before. --]
           [#set lastBlockStartIndex = state_index]
           [#set useIf = true]
         [/#if]
            [#if useIf]if[#else]else if[/#if] ([@NfaStateCondition state /]) {
      [/#if]
      [#if state.nextStateIndex >= 0]
                nextStates.Set(${state.nextStateIndex});
      [/#if]
      [#if !state_has_next || !state.moveRanges.equals(states[state_index+1].moveRanges)]
        [#-- We've reached the end of the block. --]
          [#var type = state.nextStateType]
          [#if type??]
                if (validTypes.Contains(${TT}${type.label})) {
                    type = ${TT}${type.label};
                }
          [/#if]
            }
      [/#if]
    [/#list]
            return type;
        }

[/#macro]

[#--
  Generates the code for an NFA state transition
  within a composite state. This code is a bit tricky
  because it consolidates more than one condition in
  a single conditional block. The jumpOut parameter says
  whether we can just jump out of the method.
  (This is based on whether any of the moveRanges
  for later states overlap. If not, we can jump out.)
--]
[#macro GenerateStateMove nfaState isFirstOfGroup isLastOfGroup useElif=false]
  [#var nextState = nfaState.nextState.canonicalState]
  [#var type = nfaState.nextState.type]
  [#if isFirstOfGroup]
            [#if useElif]else if[#else]if[/#if] ([@NfaStateCondition nfaState /]) {
  [/#if]
  [#if nextState.index >= 0]
                nextStates.Set(${nextState.index});
  [/#if]
  [#if isLastOfGroup]
    [#if type??]
                if (validTypes.Contains(${TT}${type.label})) {
                    type = ${TT}${type.label};
                }
    [/#if]
            }
  [/#if]
[/#macro]

[#--
  Generate the code for a simple (non-composite) NFA state
--]
[#macro GenerateSimpleNfaMethod nfaState]
        private static TokenType? ${nfaState.methodName}(int ch, BitSet nextStates, HashSet<TokenType> validTypes) {
[#var type = nfaState.nextStateType]
            if ([@NfaStateCondition nfaState /]) {
  [#if nfaState.nextStateIndex >= 0]
                nextStates.Set(${nfaState.nextStateIndex});
  [/#if]
[#if type??]
                if (validTypes.Contains(${TT}${type.label})) {
                    return ${TT}${type.label};
                }
[/#if]
            }
            return null;
        }

[/#macro]

[#--
Generate the condition part of the NFA state transition
If the size of the moveRanges vector is greater than NFA_RANGE_THRESHOLD
it uses the canned binary search routine. For the smaller moveRanges
it just generates the inline conditional expression
--]
[#macro NfaStateCondition nfaState]
    [#if nfaState.moveRanges?size < NFA_RANGE_THRESHOLD]
      [@RangesCondition nfaState.moveRanges /][#t]
    [#elseif nfaState.hasAsciiMoves && nfaState.hasNonAsciiMoves]
      ([@RangesCondition nfaState.asciiMoveRanges/]) || ((ch >= (char) 128) && CheckIntervals(${nfaState.movesArrayName}, ch))[#t]
    [#else]
      CheckIntervals(${nfaState.movesArrayName}, ch)[#t]
    [/#if]
[/#macro]

[#--
This is a recursive macro that generates the code corresponding
to the accepting condition for an NFA state. It is used
if NFA state's moveRanges array is smaller than NFA_RANGE_THRESHOLD
(which is set to 16 for now)
--]
[#macro RangesCondition moveRanges]
    [#var left = moveRanges[0], right = moveRanges[1]]
    [#var displayLeft = grammar.utils.displayChar(left), displayRight = grammar.utils.displayChar(right)]
    [#var singleChar = left == right]
    [#if moveRanges?size==2]
       [#if singleChar]
          ch == ${displayLeft}[#t]
       [#elseif left +1 == right]
          ch == ${displayLeft} || ch == ${displayRight}[#t]
       [#elseif left > 0]
          ch >= ${displayLeft}[#t]
          [#if right < 1114111]
 && ch <= ${displayRight} [#rt]
          [/#if]
       [#else]
           ch <= ${displayRight} [#t]
       [/#if]
    [#else]
       ([@RangesCondition moveRanges[0..1]/]) || ([@RangesCondition moveRanges[2..moveRanges?size-1]/])[#t]
    [/#if]
[/#macro]

[#var csPackage = grammar.utils.getPreprocessorSymbol('cs.package', grammar.parserPackage) ]
namespace ${csPackage} {
    using System;
    using System.Collections.Generic;
    using System.Diagnostics;
    using System.IO;
    using System.Text;
    using System.Text.RegularExpressions;
${grammar.utils.translateLexerImports()}

    public class Lexer[#if useLogging!false] : IObservable<LogInfo>[/#if] {
        // Lexer fields and properties (non-NFA-related)

        const int DEFAULT_TAB_SIZE = ${grammar.tabSize};

        internal static readonly Token DummyStartToken, Ignored, Skipped;

        public string InputSource { get; internal set; }
        private readonly string _content;
        private readonly int _contentLength;

[#if grammar.lexerUsesParser]
        internal Parser Parser { get; internal set; }
[/#if]

        // The starting line and column, usually 1,1
        // that is used to report a file position
        // in 1-based line/column terms
        private int startingLine, startingColumn;

[#if lexerData.hasLexicalStateTransitions]
        // A lookup for lexical state transitions triggered by a certain token type
        private static Dictionary<TokenType, LexicalState> tokenTypeToLexicalStateMap = new Dictionary<TokenType, LexicalState>();
[/#if]

        // Token types that are "regular" tokens that participate in parsing,
        // i.e. declared as TOKEN
        [@EnumSet "regularTokens" lexerData.regularTokens.tokenNames 8 /]
        // Token types that do not participate in parsing, a.k.a. "special" tokens in legacy JavaCC,
        // i.e. declared as UNPARSED (or SPECIAL_TOKEN)
        [@EnumSet "unparsedTokens" lexerData.unparsedTokens.tokenNames 8 /]
        [#-- // Tokens that are skipped, i.e. SKIP --]
        [@EnumSet "skippedTokens" lexerData.skippedTokens.tokenNames 8 /]
        // Tokens that correspond to a MORE, i.e. that are pending
        // additional input
        [@EnumSet "moreTokens" lexerData.moreTokens.tokenNames 8 /]

        // NFA code and data

        // The functional interface that represents
        // the acceptance method of an NFA state
        private delegate TokenType? NfaFunction(int ch, BitSet bs, HashSet<TokenType> validTypes);

[#if multipleLexicalStates]
        // A lookup of the NFA function tables for the respective lexical states.
        private static Dictionary<LexicalState, NfaFunction[]> functionTableMap = new Dictionary<LexicalState, NfaFunction[]>();
[#else]
        [#-- We don't need the above lookup if there is only one lexical state.--]
        private static NfaFunction[] nfaFunctions;
[/#if]

        // Just use the canned binary search to check whether the char
        // is in one of the intervals
        private static bool CheckIntervals(int[] ranges, int ch) {
            var temp = System.Array.BinarySearch(ranges, ch);
            return temp >=0 || temp % 2 == 0;
        }

[#list lexerData.lexicalStates as lexicalState]
[@GenerateStateCode lexicalState/]
[/#list]

        [#-- Compute the maximum size of state bitsets --]
[#if !multipleLexicalStates]
            private const int MaxStates = ${lexerData.lexicalStates.get(0).allNfaStates.size()};
[#else]
            private static int MaxStates = Utils.MaxOf(
[#list lexerData.lexicalStates as state]
                ${state.allNfaStates.size()}[#if state_has_next],[/#if]
[/#list]
            );
[/#if]

        // The following two BitSets are used to store
        // the current active NFA states in the core tokenization loop
        private BitSet _nextStates = new BitSet(MaxStates), _currentStates = new BitSet(MaxStates);

        internal HashSet<TokenType> ActiveTokenTypes = Utils.EnumSet(
[#list grammar.lexerData.regularExpressions as regexp]
            TokenType.${regexp.label}[#if regexp_has_next],[/#if]
[/#list]
        );

        private LexicalState _lexicalState;
        private readonly int[] _lineOffsets;   // offsets to the beginnings of lines
        private readonly Token[] _tokenLocationTable;
        private int _bufferPosition;
        private readonly BitSet _tokenOffsets;

        //  A Bitset that stores the line numbers that
        // contain either hard tabs or extended (beyond 0xFFFF) unicode
        // characters.
        private readonly BitSet _needToCalculateColumns;

        public LexicalState LexicalState => _lexicalState;

        // constructors

        static Lexer() {
[#list lexerData.lexicalStates as lexicalState]
            NFA_FUNCTIONS_${lexicalState.name}Init();
[/#list]
[#if lexerData.hasLexicalStateTransitions]
            // Generate the map for lexical state transitions from the various token types
  [#list grammar.lexerData.regularExpressions as regexp]
    [#if !regexp.newLexicalState?is_null]
            tokenTypeToLexicalStateMap[TokenType.${regexp.label}] = LexicalState.${regexp.newLexicalState.name};
    [/#if]
  [/#list]
[/#if]
            DummyStartToken = new InvalidToken(null, 0, 0);
            Ignored = new InvalidToken(null, 0, 0);
            Skipped = new InvalidToken(null, 0, 0);
            Ignored.IsUnparsed = true;
            Skipped.IsUnparsed = true;
        }

[#if useLogging!false]
        private readonly IList<IObserver<LogInfo>> observers = new List<IObserver<LogInfo>>();
[/#if]

        public Lexer(string inputSource, LexicalState lexState = LexicalState.${lexerData.lexicalStates[0].name}, int line = 1, int column = 1) {
            InputSource = inputSource;
            var input = InputText(inputSource);
            _content = MungeContent(input, ${PRESERVE_TABS}, ${PRESERVE_LINE_ENDINGS}, ${JAVA_UNICODE_ESCAPE}, ${ENSURE_FINAL_EOL});
            _contentLength = _content.Length;
            _needToCalculateColumns = new BitSet(_contentLength + 1);
            _lineOffsets = CreateLineOffsetsTable(_content);
            _tokenLocationTable = new Token[_contentLength + 1];
            _tokenOffsets = new BitSet(_contentLength + 1);
            startingLine = line;
            startingColumn = column;
[#if grammar.deactivatedTokens?size>0 || grammar.extraTokens?size >0]
  [#list grammar.deactivatedTokens as token]
            ActiveTokenTypes.Remove(${CU.TT}${token});
  [/#list]
  [#list grammar.extraTokenNames as token]
            regularTokens.Add(${CU.TT}${token});
  [/#list]
[/#if]
${grammar.utils.translateLexerInitializers(injector)}
            SwitchTo(lexState);
        }

        private static readonly Regex PythonCodingPattern = new Regex(@"^[ \t\f]*#.*\bcoding[:=][ \t]*([-_.a-zA-Z0-9]+)");
        private static readonly UTF8Encoding Utf8 = new UTF8Encoding(true);
        private static readonly UnicodeEncoding Utf16Le = new UnicodeEncoding(false, true, true);
        private static readonly UnicodeEncoding Utf16Be = new UnicodeEncoding(true, true, true);
        private static readonly UTF32Encoding Utf32Le = new UTF32Encoding(false, true, true);
        private static readonly UTF32Encoding Utf32Be = new UTF32Encoding(true, true, true);

        private bool PreamblesEqual(Span<byte> data, byte[] preamble) {
            for (int i = preamble.Length - 1; i >= 0; i--) {
                if (data[i] != preamble[i]) {
                    return false;
                }
            }
            return true;
        }

/*
        private static string TryDecode(Encoding encoding, byte[] bytes)
        {
            var byteIndex = 0;
            var byteCount = bytes.Length;
            var chars = new char[byteCount];
            var charIndex = 0;
            var charCount = byteCount;
            int bytesUsed, charsUsed;
            bool completed;

            var decoder = encoding.GetDecoder();
            while (true)
            {
                decoder.Convert(bytes, byteIndex, byteCount,
                        chars, charIndex, charCount,
                        false, out bytesUsed, out charsUsed, out completed);
                if (completed)
                {
                    break;
                }

                throw new ArgumentException();
            }
            return new string(chars, 0, charsUsed);
        }
 */
        private String InputText(string path) {
            FileStream fs;

            try {
                fs = new FileStream(path, FileMode.Open);
            }
            catch (IOException) {
                // assume we were passed source code.
                return path;
            }
            var fileLen = (int) fs.Length;
            var bytes = new byte[fileLen];
            var bomLen = 3;
            Encoding encoding;
            var allBytes = new Span<byte>(bytes);
            Span<byte> bomBytes;
            Span<byte> foundBom = null;

            fs.Read(bytes, 0, fileLen);

            if (fileLen <= bomLen) {
                encoding = Utf8;
            }
            else if (PreamblesEqual(bomBytes = allBytes[..bomLen], Utf8.GetPreamble())) {
                encoding = Utf8;
                foundBom = bomBytes;
            }
            else if (PreamblesEqual(bomBytes = allBytes[..(bomLen = 2)], Utf16Le.GetPreamble())) {
                encoding = Utf16Le;
                foundBom = bomBytes;
            }
            else if (PreamblesEqual(bomBytes, Utf16Be.GetPreamble())) {
                encoding = Utf16Be;
                foundBom = bomBytes;
            }
            else if (PreamblesEqual(bomBytes = allBytes[(bomLen = 4)..], Utf32Le.GetPreamble())) {
                encoding = Utf32Le;
                foundBom = bomBytes;
            }
            else if (PreamblesEqual(bomBytes, Utf32Be.GetPreamble())) {
                encoding = Utf32Be;
                foundBom = bomBytes;
            }
            else {
                encoding = Utf8;
                if (path.EndsWith(".py")) {
                    // Look for coding declared in first two lines
                    var pos = System.Array.IndexOf(bytes, '\n');
                    if (pos > 0) {
                        var s = Utf8.GetString(allBytes.Slice(0, pos));
                        var m = PythonCodingPattern.Match(s);

                        if (m.Success) {
                            s = m.Groups[1].Value;
                            encoding = Encoding.GetEncoding(s);
                        }
                        else {
                            pos = System.Array.IndexOf(bytes, '\n', pos + 1);
                            if (pos > 0) {
                                s = Utf8.GetString(allBytes.Slice(0, pos));
                                m = PythonCodingPattern.Match(s);
                                if (m.Success) {
                                    s = m.Groups[1].Value;
                                    encoding = Encoding.GetEncoding(s);
                                }
                            }
                        }
                    }
                }
            }
            var rest = (foundBom == null) ? allBytes : allBytes[bomLen..];
            //return TryDecode(encoding, rest.ToArray());
            return encoding.GetString(rest);
        }

        private static readonly int[] EmptyInt = new int[] { 0 };

        private int[] CreateLineOffsetsTable(string content) {
            if (content.Length == 0) {
                return EmptyInt;
            }
            var lineCount = 0;
            var length = content.Length;
            for (var i = 0; i < length; i++) {
                var ch = content[i];
                if (ch == '\t' || char.IsHighSurrogate(ch)) {
                    _needToCalculateColumns.Set(lineCount);
                }
                if (ch == '\n') {
                    lineCount++;
                }
            }
            if (content[^1] != '\n') {
                lineCount++;
            }
            var lineOffsets = new int[lineCount];
            lineOffsets[0] = 0;
            var index = 1;
            for (var i = 0; i < length; i++) {
                var ch = content[i];
                if (ch == '\n') {
                    if (i + 1 == length)
                        break;
                    lineOffsets[index++] = i + 1;
                }
            }
            return lineOffsets;
        }

[#if useLogging!false]
        public IDisposable Subscribe(IObserver<LogInfo> observer)
        {
            if (!observers.Contains(observer)) {
                observers.Add(observer);
            }
            return new Unsubscriber<LogInfo>(observers, observer);
        }

        internal void Log(LogLevel level, string message, params object[] arguments) {
            var info = new LogInfo(level, message, arguments);
            foreach (var observer in observers) {
                observer.OnNext(info);
            }
        }
[/#if]
        //
        // Switch to specified lexical state.
        //
        public bool SwitchTo(LexicalState lexState) {
            if (_lexicalState != lexState) {
                _lexicalState = lexState;
                return true;
            }
            return false;
        }

[#if lexerData.hasLexicalStateTransitions]
        bool DoLexicalStateSwitch(TokenType tokenType) {
            if (!tokenTypeToLexicalStateMap.ContainsKey(tokenType)) {
                return false;
            }
            LexicalState newState = tokenTypeToLexicalStateMap[tokenType];
            return SwitchTo(newState);
        }
[/#if]

        private Token GetNextToken() {
            Token invalidToken = null;
            Token token = NextToken();

            while (token is InvalidToken) {
                if (invalidToken == null) {
                    invalidToken = token;
                }
                else {
                    invalidToken.EndOffset = token.EndOffset;
                }
                token = NextToken();
            }
            if (invalidToken != null) {
                CacheToken(invalidToken);
            }
            CacheToken(token);
            return (invalidToken != null) ? invalidToken : token;
        }

        /**
        * The public method for getting the next token.
        * If the tok parameter is null, it just tokenizes
        * starting at the internal _bufferPosition
        * Otherwise, it checks whether we have already cached
        * the token after this one. If not, it finally goes
        * to the NFA machinery
        */
        public Token GetNextToken(Token tok) {
            if(tok == null) {
                return GetNextToken();
            }
            Token cachedToken = tok.NextCachedToken;
            // If the cached next token is not currently active, we
            // throw it away and go back to the lexer
            if (cachedToken != null && !ActiveTokenTypes.Contains(cachedToken.Type)) {
                Reset(tok);
                cachedToken = null;
            }
            return cachedToken != null ? cachedToken : GetNextToken(tok.EndOffset);
        }

        /**
        * A lower level method to tokenize, that takes the absolute
        * offset into the _content buffer as a parameter
        * @param offset where to start
        * @return the token that results from scanning from the given starting point
        */
        public Token GetNextToken(int offset) {
            GoTo(offset);
            return GetNextToken();
        }

        // The main method to invoke the NFA machinery
        private Token NextToken() {
            Token matchedToken = null;
            var inMore = false;
            var tokenBeginOffset = _bufferPosition;
            var firstChar = 0;
            // The core tokenization loop
            while (matchedToken == null) {
                var codeUnitsRead = 0;
                var matchedPos = 0;
                int curChar;
                TokenType? matchedType = null;
                var reachedEnd = false;
                if (inMore) {
                    curChar = ReadChar();
                    if (curChar < 0) reachedEnd = true;
                }
                else {
                    tokenBeginOffset = _bufferPosition;
                    firstChar = curChar = ReadChar();
                    if (curChar < 0) {
                        matchedType = TokenType.EOF;
                        reachedEnd = true;
                    }
                }
[#if multipleLexicalStates]
                // Get the NFA function table current lexical state
                // There is some possibility that there was a lexical state change
                // since the last iteration of this loop!
[/#if]
                NfaFunction[] functions = GetFunctionTable(_lexicalState);
                // the core NFA loop
                if (!reachedEnd) do {
                    // Holder for the new type (if any) matched on this iteration
                    TokenType? newType = null;
                    if (codeUnitsRead > 0) {
                        // What was _nextStates on the last iteration
                        // is now the _currentStates!
                        (_currentStates, _nextStates) = (_nextStates, _currentStates);
                        var c = ReadChar();
                        if (c >= 0) {
                            curChar = c;
                        }
                        else {
                            // reachedEnd = true;
                            break;
                        }
                    }
                    _nextStates.Clear();
                    if (codeUnitsRead == 0) {
                        TokenType? returnedType = functions[0](curChar, _nextStates, ActiveTokenTypes);
                        if (returnedType != null) {
                            newType = returnedType;
                        }
                    }
                    else {
                        int nextActive = _currentStates.NextSetBit(0);
                        while (nextActive != -1) {
                            TokenType? returnedType = functions[nextActive](curChar, _nextStates, ActiveTokenTypes);
                            if ((returnedType != null) && ((newType == null) || ((int) returnedType.Value < (int) newType.Value))) {
                                newType = returnedType;
                            }
                            nextActive = _currentStates.NextSetBit(nextActive + 1);
                        }
                    }
                    ++codeUnitsRead;
                    if (curChar >= 0xFFFF) {
                        ++codeUnitsRead;
                    }
                    if (newType != null) {
                        matchedType = newType;
                        inMore = moreTokens.Contains(matchedType.Value);
                        matchedPos = codeUnitsRead;
                    }
                } while (!_nextStates.IsEmpty);
                if (matchedType == null) {
                    _bufferPosition = tokenBeginOffset + 1;
                    if (firstChar > 0xFFFF) {
                        ++_bufferPosition;
                    }
                    return new InvalidToken(this, tokenBeginOffset, _bufferPosition);
                }
                _bufferPosition -= (codeUnitsRead - matchedPos);
                if (skippedTokens.Contains((TokenType) matchedType)) {
                    for (int i = tokenBeginOffset; i < _bufferPosition; i++) {
                        if (_tokenLocationTable[i] != Ignored) {
                            _tokenLocationTable[i] = Skipped;
                        }
                    }
                }
                if (regularTokens.Contains((TokenType) matchedType) || unparsedTokens.Contains((TokenType) matchedType)) {
                    matchedToken = Token.NewToken((TokenType) matchedType,
                                                  this,
                                                  tokenBeginOffset,
                                                  _bufferPosition);
                    matchedToken.IsUnparsed = !regularTokens.Contains((TokenType) matchedType);
                }
     [#if lexerData.hasTokenActions]
                matchedToken = TokenLexicalActions(matchedToken, matchedType);
     [/#if]
     [#if lexerData.hasLexicalStateTransitions]
                DoLexicalStateSwitch(matchedType.Value);
     [/#if]
            }
 [#list grammar.lexerTokenHooks as tokenHookMethodName]
    [#if tokenHookMethodName = "CommonTokenAction"]
                ${tokenHookMethodName}(matchedToken);
    [#else]
                matchedToken = ${tokenHookMethodName}(matchedToken);
    [/#if]
 [/#list]
            return matchedToken;
        }

        // Reset the token source input
        // to just after the Token passed in.
        internal void Reset(Token t, LexicalState? state = null) {
[#list grammar.resetTokenHooks as resetTokenHookMethodName]
            ${grammar.utils.translateIdentifier(resetTokenHookMethodName)}(t);
[/#list]
            GoTo(t.EndOffset);
            UncacheTokens(t);
            if (state != null) {
                SwitchTo(state.Value);
            }
[#if lexerData.hasLexicalStateTransitions]
            else {
                DoLexicalStateSwitch(t.Type);
            }
[/#if]
        }

[#if lexerData.hasTokenActions]
        private Token TokenLexicalActions(Token matchedToken, TokenType matchedType) {
            switch (matchedType) {
        [#list lexerData.regularExpressions as regexp]
                [#if regexp.codeSnippet?has_content]
            case ${regexp.label}:
${grammar.utils.translateCodeBlock(regexp.codeSnippet.javaCode, 16)}
                break;
                [/#if]
        [/#list]
            default: break;
            }
            return matchedToken;
        }
[/#if]

        private string MungeContent(string content, bool preserveTabs, bool preserveLines,
                      bool unicodeEscape, bool ensureFinalEol)
        {
            StringBuilder buf;

            if (preserveTabs && preserveLines && !unicodeEscape) {
                if (!ensureFinalEol) return _content;
                if (content.Length == 0) {
                    content = "\n";
                }
                else {
                    int lastChar = _content[^1];
                    if (lastChar == '\n' || lastChar == '\r') return content;
                    buf = new StringBuilder(content);
                    buf.Append('\n');
                    content = buf.ToString();
                }
                return content;
            }
            buf = new StringBuilder();
            // This is just to handle tabs to spaces. If you don't have that setting set, it
            // is really unused.
            var col = 0;
            var justSawUnicodeEscape = false;
            // There might be some better way of doing this ...
            var bytes = Encoding.UTF32.GetBytes(content);
            var codePoints = new int[bytes.Length / 4];
            Buffer.BlockCopy(bytes, 0, codePoints, 0, bytes.Length);
            for (var index = 0; index < codePoints.Length; )
            {
                var ch = codePoints[index++];
                switch (ch)
                {
                    case '\\' when unicodeEscape && index < codePoints.Length:
                    {
                        ch = codePoints[index++];
                        if (ch != 'u') {
                            justSawUnicodeEscape = false;
                            buf.Append('\\');
                            buf.Append((char) ch);
                            if (ch == '\n')
                                col = 0;
                            else
                                col += 2;
                        } else {
                            while (codePoints[index] == 'u') {
                                index++;
                                // col++;
                            }
                            var hexBuf = new StringBuilder(4);
                            for (var i = 0; i < 4; i++) hexBuf.Append((char) codePoints[index++]);
                            var current = (char) Convert.ToInt32(hexBuf.ToString(), 16);
                            var last = buf.Length > 0 ? buf[^1] : (char) 0;
                            if (justSawUnicodeEscape && char.IsSurrogatePair(last, current)) {
                                buf.Length -= 1;
                                --col;
                                buf.Append(char.ConvertToUtf32(last, current));
                                justSawUnicodeEscape = false;
                            } else {
                                buf.Append(current);
                                justSawUnicodeEscape = true;
                            }
                            // col +=6;
                            ++col;
                            // We're not going to be trying to track line/column information relative to the original content
                            // with tabs or unicode escape, so we just increment 1, not 6
                        }

                        break;
                    }
                    case '\r' when !preserveLines:
                    {
                        justSawUnicodeEscape = false;
                        buf.Append('\n');
                        if (index < codePoints.Length) {
                            ch = codePoints[index++];
                            if (ch != '\n') {
                                buf.Append((char) ch);
                                ++col;
                            }
                            else
                            {
                                col = 0;
                            }
                        }
                        break;
                    }
                    case '\t' when !preserveTabs:
                    {
                        justSawUnicodeEscape = false;
                        int spacesToAdd = ${grammar.tabSize} - col % ${grammar.tabSize};
                        for (int i = 0; i < spacesToAdd; i++) {
                            buf.Append(' ');
                            col++;
                        }

                        break;
                    }
                    default:
                    {
                        justSawUnicodeEscape = false;
                        buf.Append(char.ConvertFromUtf32(ch));
                        if (ch == '\n') {
                            col = 0;
                        } else
                            col++;

                        break;
                    }
                }
            }

            if (!ensureFinalEol) return buf.ToString();
            if (buf.Length == 0) {
                return "\n";
            }
            var lc = buf[^1];
            if (lc != '\n' && lc!='\r') buf.Append('\n');
            return buf.ToString();
        }

        private void GoTo(int offset) {
            while (_tokenLocationTable[offset] == Ignored && offset < _contentLength) {
                ++offset;
            }
            _bufferPosition = offset;
        }

        /**
        * return the line length in code _units_
        */
        private int GetLineLength(int lineNumber) {
            int startOffset = GetLineStartOffset(lineNumber);
            int endOffset = GetLineEndOffset(lineNumber);
            return 1 + endOffset - startOffset;
        }

        /*
         * The offset of the start of the given line. This is in code units
         */
        private int GetLineStartOffset(int lineNumber) {
            int realLineNumber = lineNumber - startingLine;
            if (realLineNumber <= 0) {
                return 0;
            }
            if (realLineNumber >= _lineOffsets.Length) {
                return _contentLength;
            }
            return _lineOffsets[realLineNumber];
        }

        /*
         * The offset of the end of the given line. This is in code units.
         */
        private int GetLineEndOffset(int lineNumber) {
            int realLineNumber = lineNumber - startingLine;
            if (realLineNumber < 0) {
                return 0;
            }
            if (realLineNumber >= _lineOffsets.Length) {
                return _contentLength;
            }
            if (realLineNumber == _lineOffsets.Length - 1) {
                return _contentLength - 1;
            }
            return _lineOffsets[realLineNumber + 1] - 1;
        }

        /**
        * The number of supplementary unicode characters in the specified
        * offset range. The range is expressed in code units
        */
        private int NumSupplementaryCharactersInRange(int start, int end) {
            int result = 0;
            while (start < end - 1) {
                if (char.IsHighSurrogate(_content[start++])) {
                    if (char.IsLowSurrogate(_content[start])) {
                        start++;
                        result++;
                    }
                }
            }
            return result;
        }

        private int ReadChar() {
            while (_tokenLocationTable[_bufferPosition] == Ignored && _bufferPosition < _contentLength) {
                ++_bufferPosition;
            }
            if (_bufferPosition >= _contentLength) {
                return -1;
            }
            char ch = _content[_bufferPosition++];
            if (char.IsHighSurrogate(ch) && _bufferPosition < _contentLength) {
                char nextChar = _content[_bufferPosition];
                if (char.IsLowSurrogate(nextChar)) {
                    ++_bufferPosition;
                    return char.ConvertToUtf32(ch, nextChar);
                }
            }
            return ch;
        }

        /**
        * This is used in conjunction with having a preprocessor.
        * We set which lines are actually parsed lines and the
        * unset ones are ignored.
        * @param parsedLines a #java.util.BitSet that holds which lines
        * are parsed (i.e. not ignored)
        */
        public void SetParsedLines(BitSet parsedLines) {
            for (int i = 0; i < _lineOffsets.Length; i++) {
                if (!parsedLines[i + 1]) {
                    int lineOffset = _lineOffsets[i];
                    int nextLineOffset = i < _lineOffsets.Length -1 ? _lineOffsets[i+1] : _contentLength;
                    for (int offset = lineOffset; offset < nextLineOffset; offset++) {
                        _tokenLocationTable[offset] = Ignored;
                    }
                }
            }
        }

        /**
        * @return the line number from the absolute offset passed in as a parameter
        */
        internal int GetLineFromOffset(int pos) {
            if (pos >= _contentLength) {
                if (_content[_contentLength - 1] == '\n') {
                    return startingLine + _lineOffsets.Length;
                }
                return startingLine + _lineOffsets.Length - 1;
            }
            int bsearchResult = System.Array.BinarySearch(_lineOffsets, pos);
            if (bsearchResult >= 0) {
                return startingLine + bsearchResult;
            }
            return startingLine -(bsearchResult + 2);
        }

        internal int GetCodePointColumnFromOffset(int pos) {
            if (pos >= _contentLength) return 1;
            if (pos == 0) return startingColumn;
            var line = GetLineFromOffset(pos) - startingLine;
            var lineStart = _lineOffsets[line];
            var startColumnAdjustment = line > 0 ? 1 : startingColumn;
            var unadjustedColumn = pos - lineStart + startColumnAdjustment;
            if (!_needToCalculateColumns[line]) {
                return unadjustedColumn;
            }
            if (char.IsLowSurrogate(_content[pos])) --pos;
            var result = startColumnAdjustment;
            for (int i = lineStart; i < pos; i++) {
                var ch = _content[i];
                if (ch == '\t') {
                    result += DEFAULT_TAB_SIZE - (result - 1) % DEFAULT_TAB_SIZE;
                }
                else if (char.IsHighSurrogate(ch)) {
                    ++result;
                    ++i;
                }
                else {
                    ++result;
                }
            }
            return result;
        }

        /**
        * @return the text between startOffset (inclusive)
        * and endOffset(exclusive)
        */
        internal string GetText(int startOffset, int endOffset) {
            StringBuilder buf = new StringBuilder();
            for (int offset = startOffset; offset < endOffset; offset++) {
                if (_tokenLocationTable[offset] != Ignored) {
                    buf.Append(_content[offset]);
                }
            }
            return buf.ToString();
        }

        internal void CacheToken(Token tok) {
[#if !grammar.minimalToken]
            if (tok.isInserted) {
                Token next = tok.NextCachedToken;
                if (next != null) CacheToken(next);
                return;
            }
[/#if]
            int offset = tok.BeginOffset;
            if (_tokenLocationTable[offset] != Ignored) {
                _tokenOffsets.Set(offset);
                _tokenLocationTable[offset] = tok;
            }
        }

        void UncacheTokens(Token lastToken) {
            int endOffset = lastToken.EndOffset;
            if (endOffset < _tokenOffsets.Length) {
                _tokenOffsets.Clear(lastToken.EndOffset, _tokenOffsets.Length);
            }
[#if !grammar.minimalToken]
            lastToken.UnsetAppendedToken();
[/#if]
        }

        internal Token NextCachedToken(int offset) {
            int nextOffset = _tokenOffsets.NextSetBit(offset);
            return nextOffset == -1 ? null : _tokenLocationTable[nextOffset];
        }

        internal Token PreviousCachedToken(int offset) {
            int prevOffset = _tokenOffsets.PreviousSetBit(offset - 1);
            return prevOffset == -1 ? null : _tokenLocationTable[prevOffset];
        }

        private static NfaFunction[] GetFunctionTable(LexicalState lexicalState) {
[#if multipleLexicalStates]
            return functionTableMap[lexicalState];
[#else]
            // We only have one lexical state in this case, so we return that!
            return nfaFunctions;
[/#if]
        }

        protected void SetRegionIgnore(int start, int end) {
            for (int i = start; i< end; i++) {
                _tokenLocationTable[i] = Ignored;
            }
            _tokenOffsets.Clear(start, end);
        }

        protected bool AtLineStart(Token tok) {
            int offset = tok.BeginOffset;
            while (offset > 0) {
                --offset;
                char c = _content[offset];
                if (!Char.IsWhiteSpace(c)) return false;
                if (c == '\n') break;
            }
            return true;
        }

        protected string GetLine(Token tok) {
            int lineNum = tok.BeginLine;
            return GetText(GetLineStartOffset(lineNum), GetLineEndOffset(lineNum) + 1);
        }

        protected void SetLineSkipped(Token tok) {
            int lineNum = tok.BeginLine;
            int start = GetLineStartOffset(lineNum);
            int end = GetLineStartOffset(lineNum+1);
            SetRegionIgnore(start, end);
            tok.BeginOffset = start;
            tok.EndOffset = end;
        }

${grammar.utils.translateLexerInjections(injector, true)}

${grammar.utils.translateLexerInjections(injector, false)}
    }
}
