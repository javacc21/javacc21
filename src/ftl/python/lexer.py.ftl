[#ftl strict_vars=true]
[#--
  Copyright (C) 2008-2020 Jonathan Revusky, revusky@javacc.com
  Copyright (C) 2021-2022 Vinay Sajip, vinay_sajip@yahoo.co.uk
  All rights reserved.

  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions are met:

      * Redistributions of source code must retain the above copyright
        notices, this list of conditions and the following disclaimer.
      * Redistributions in binary form must reproduce the above copyright
        notice, this list of conditions and the following disclaimer in
        the documentation and/or other materials provided with the
        distribution.
      * None of the names Jonathan Revusky, Vinay Sajip, Sun
        Microsystems, Inc. nor the names of any contributors may be
        used to endorse or promote products derived from this software
        without specific prior written permission.

  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
  THE POSSIBILITY OF SUCH DAMAGE.
--]
# Parser lexing package. Generated by ${generated_by}. Do not edit.
[#import "common_utils.inc.ftl" as CU]

import bisect
import logging
import re

from .tokens import TokenType, LexicalState, InvalidToken, new_token
[#if grammar.extraTokens?size > 0]
  [#list grammar.extraTokenNames as tokenName]
from .tokens import ${grammar.extraTokens[tokenName]}
  [/#list]
[/#if]
from .utils import as_chr, _List, EMPTY_SET

# See if an accelerated BitSet is available.
try:
    from _bitset import BitSet
    _fast_bitset = True
except ImportError:
    from .utils import BitSet
    _fast_bitset = False

[#var NFA_RANGE_THRESHOLD = 16]
[#var MAX_INT=2147483647]
[#var lexerData=grammar.lexerData]
[#var multipleLexicalStates = lexerData.lexicalStates.size() > 1]
[#var TT = "TokenType."]

logger = logging.getLogger(__name__)

#
# Hack to allow token types to be referenced in snippets without
# qualifying
#
globals().update(TokenType.__members__)

# NFA code and data
[#if multipleLexicalStates]
# A mapping from lexical state to NFA functions for that state.
[#-- We only need the mapping if there is more than one lexical state.--]
function_table_map = {}
[/#if]

[#list lexerData.lexicalStates as lexicalState]
[@GenerateStateCode lexicalState/]
[/#list]

# Just use binary search to check whether the char is in one of the
# intervals
def check_intervals(ranges, ch):
    index = bisect.bisect_left(ranges, ch)
    n = len(ranges)
    if index < n:
        if index % 2 == 0:
            if index < (n - 1):
                return ranges[index] <= ch <= ranges[index + 1]
        elif index > 0:
            return ranges[index - 1] <= ch <= ranges[index]
    return False

[#--
  Generate all the NFA transition code
  for the given lexical state
--]
[#macro GenerateStateCode lexicalState]
[#list lexicalState.allNfaStates as nfaState]
[#if nfaState.moveRanges.size() >= NFA_RANGE_THRESHOLD]
[@GenerateMoveArray nfaState/]
[/#if]
[@GenerateNfaStateMethod nfaState/]
[/#list]

def NFA_FUNCTIONS_${lexicalState.name}_init():
[#--
    In theory this could be initialized as a normal list, but it's
    not clear if state.index is always the same as state_index

    Update: it is, see https://github.com/javacc21/javacc21/issues/72

    functions = [None] * ${lexicalState.allNfaStates.size()}
    [#list lexicalState.allNfaStates as state]
    functions[${state.index}] = ${state.methodName}
    [/#list]
--]
    functions = [
[#list lexicalState.allNfaStates as state]
        ${state.methodName}[#if state_has_next],[/#if]
[/#list]
    ]
[#if multipleLexicalStates]
    function_table_map[LexicalState.${lexicalState.name}] = functions
[#else]
    return functions
[/#if]

[#if multipleLexicalStates]
NFA_FUNCTIONS_${lexicalState.name}_init()
[#else]
nfa_functions = NFA_FUNCTIONS_${lexicalState.name}_init()
[/#if]

[/#macro]

[#--
   Generate the array representing the characters
   that this NfaState "accepts".
   This corresponds to the moveRanges field in
   com.javacc.core.NfaState
--]
[#macro GenerateMoveArray nfaState]
[#var moveRanges = nfaState.moveRanges]
[#var arrayName = nfaState.movesArrayName]
[#-- No need to create an array and populate one by one - just
     initialize normally

def ${arrayName}_init():
     it!
    result = [0] * ${nfaState.moveRanges.size()}
[#list nfaState.moveRanges as char]
    result[${char_index}] = ${grammar.utils.displayChar(char)}
    return result
${arrayName} = ${arrayName}_init()
[/#list--]
${arrayName} = [
[#list nfaState.moveRanges as char]
    ${grammar.utils.displayChar(char)}[#if char_has_next],[/#if]
[/#list]
]
[/#macro]

[#--
   Generate the method that represents the transition
   (or transitions if this is a CompositeStateSet)
   that correspond to an instanceof com.javacc.core.NfaState
--]
[#macro GenerateNfaStateMethod nfaState]
  [#if !nfaState.composite]
     [@GenerateSimpleNfaMethod nfaState/]
  [#else]
def ${nfaState.methodName}(ch, next_states, valid_types):
    [#var states = nfaState.orderedStates]
    [#-- sometimes set in the code below --]
    type = None
    [#var useElif = false]
    [#list states as state]
      [#var isFirstOfGroup=true, isLastOfGroup=true]
      [#if state_index!=0]
         [#set isFirstOfGroup = !states[state_index-1].moveRanges.equals(state.moveRanges)]
      [/#if]
      [#if state_has_next]
         [#set isLastOfGroup = !states[state_index+1].moveRanges.equals(state.moveRanges)]
      [/#if]
      [@GenerateStateMove state isFirstOfGroup isLastOfGroup useElif /]
      [#if state_has_next && isLastOfGroup && !states[state_index+1].overlaps(states.subList(0, state_index+1))]
        [#set useElif = true]
      [#else]
        [#set useElif = false]
      [/#if]
    [/#list]
    return type

  [/#if]
[/#macro]

[#--
  Generates the code for an NFA state transition
  within a composite state. This code is a bit tricky
  because it consolidates more than one condition in
  a single conditional block. The jumpOut parameter says
  whether we can just jump out of the method.
  (This is based on whether any of the moveRanges
  for later states overlap. If not, we can jump out.)
--]
[#macro GenerateStateMove nfaState isFirstOfGroup isLastOfGroup useElif=false]
  [#var nextState = nfaState.nextState.canonicalState]
  [#var type = nfaState.nextState.type]
    [#if isFirstOfGroup]
    [#if useElif]elif[#else]if[/#if] [@NfaStateCondition nfaState /]:
    [/#if]
      [#if nextState.index >= 0]
        next_states.set(${nextState.index})
      [/#if]
   [#if isLastOfGroup]
      [#if type??]
        if ${TT}${type.label} in valid_types:
            type = ${TT}${type.label}
     [/#if]
   [/#if]
[/#macro]

[#--
  Generate the code for a simple (non-composite) NFA state
--]
[#macro GenerateSimpleNfaMethod nfaState]
def ${nfaState.methodName}(ch, next_states, valid_types):
[#var nextState = nfaState.nextState.canonicalState]
[#var type = nfaState.nextState.type]
    if [@NfaStateCondition nfaState /]:
        [#if nextState.index >= 0]
        next_states.set(${nextState.index})
        [/#if]
      [#if type??]
        if ${TT}${type.label} in valid_types:
            return ${TT}${type.label}
      [/#if]
    [#--return None--]

[/#macro]

[#--
Generate the condition part of the NFA state transition
If the size of the moveRanges vector is greater than NFA_RANGE_THRESHOLD
it uses the canned binary search routine. For the smaller moveRanges
it just generates the inline conditional expression
--]
[#macro NfaStateCondition nfaState]
    [#if nfaState.moveRanges?size < NFA_RANGE_THRESHOLD]
      [@RangesCondition nfaState.moveRanges /][#t]
    [#elseif nfaState.hasAsciiMoves && nfaState.hasNonAsciiMoves]
      ([@RangesCondition nfaState.asciiMoveRanges/]) or (ch >= chr(128) and check_intervals(${nfaState.movesArrayName}, ch))[#t]
    [#else]
      check_intervals(${nfaState.movesArrayName}, ch)[#t]
    [/#if]
[/#macro]

[#--
This is a recursive macro that generates the code corresponding
to the accepting condition for an NFA state. It is used
if NFA state's moveRanges array is smaller than NFA_RANGE_THRESHOLD
(which is set to 16 for now)
--]
[#macro RangesCondition moveRanges]
    [#var left = moveRanges[0], right = moveRanges[1]]
    [#var displayLeft = grammar.utils.displayChar(left), displayRight = grammar.utils.displayChar(right)]
    [#var singleChar = left == right]
    [#if moveRanges?size==2]
       [#if singleChar]
          ch == ${displayLeft}[#t]
       [#elseif left +1 == right]
          ch == ${displayLeft} or ch == ${displayRight}[#t]
       [#elseif left > 0]
          ch >= ${displayLeft}[#t]
          [#if right < 1114111]
 and ch <= ${displayRight}[#rt]
          [/#if]
       [#else]
           ch <= ${displayRight}[#t]
       [/#if]
    [#else]
       ([@RangesCondition moveRanges[0..1]/]) or ([@RangesCondition moveRanges[2..moveRanges?size-1]/])[#t]
    [/#if]
[/#macro]

# Compute the maximum size of state bitsets

[#if !multipleLexicalStates]
MAX_STATES = ${lexerData.lexicalStates.get(0).allNfaStates.size()}
[#else]
MAX_STATES = max(
[#list lexerData.lexicalStates as state]
    ${state.allNfaStates.size()}[#if state_has_next],[/#if]
[/#list]
)
[/#if]

# Lexer code and data

[#macro EnumSet varName tokenNames indent=0]
[#var is = ""?right_pad(indent)]
[#if tokenNames?size=0]
${is}self.${varName} = EMPTY_SET
[#else]
${is}self.${varName} = {
   [#list tokenNames as type]
${is}    TokenType.${type}[#if type_has_next],[/#if]
   [/#list]
${is}}
[/#if]
[/#macro]

[#list grammar.parserCodeImports as import]
   ${import}
[/#list]

[#if lexerData.hasLexicalStateTransitions]
# A mapping for lexical state transitions triggered by a certain token type (token type -> lexical state)
token_type_to_lexical_state_map = {}
[/#if]
[#var injector = grammar.injector]

def get_function_table_map(lexical_state):
    [#if multipleLexicalStates]
    return function_table_map[lexical_state]
    [#else]
    # We only have one lexical state in this case, so we return that!
    return nfa_functions
    [/#if]

[#var PRESERVE_LINE_ENDINGS=grammar.preserveLineEndings?string("True", "False")
      JAVA_UNICODE_ESCAPE= grammar.javaUnicodeEscape?string("True", "False")
      ENSURE_FINAL_EOL = grammar.ensureFinalEOL?string("True", "False")]

CODING_PATTERN = re.compile(rb'^[ \t\f]*#.*coding[:=][ \t]*([-_.a-zA-Z0-9]+)')

def _input_text(input_source):
    with open(input_source, 'rb') as f:
        text = f.read()
    if len(text) <= 3:
        encoding = 'utf-8'
    elif text[:3] == b'\xEF\xBB\xBF':
        text = text[3:]
        encoding = 'utf-8'
    elif text[:2] == b'\xFF\xFE':
        text = text[2:]
        encoding = 'utf-16le'
    elif text[:2] == b'\xFE\xFF':
        text = text[2:]
        encoding = 'utf-16be'
    elif text[:4] == b'\xFF\xFE\x00\x00':
        text = text[4:]
        encoding = 'utf-32le'
    elif text[:4] == b'\x00\x00\xFE\xFF':
        text = text[4:]
        encoding = 'utf-32be'
    else:
        # No encoding from BOM.
        encoding = 'utf-8'
        if input_source.endswith(('.py', '.pyw')):
            # Look for coding in first two lines
            parts = text.split(b'\n', 2)
            m = CODING_PATTERN.match(parts[0])
            if not m and len(parts) > 1:
                m = CODING_PATTERN.match(parts[1])
            if m:
                encoding = m.groups()[0].decode('ascii')
    return text.decode(encoding)

[#-- #var lexerClassName = grammar.lexerClassName --]
[#var lexerClassName = "Lexer"]
class ${lexerClassName}:

    __slots__ = (
        'input_source',
[#if grammar.lexerUsesParser]
        'parser',
[/#if]
        'next_states',
        'current_states',
        '_char_buf',
        'active_token_types',
        'pending_invalid_chars',
        'starting_line',
        'starting_column',
        'invalid_token',
        'previous_token',
        'regular_tokens',
        'unparsed_tokens',
        'skipped_tokens',
        'more_tokens',
        'lexical_state',
        '_line_offsets',
        '_token_offsets',
        '_token_location_table',
        'content',
        'content_len',
        '_buffer_position',
        '_dummy_start_token',
        '_ignored',
[#var injectedFields = grammar.utils.injectedLexerFieldNames(injector)]
[#if injectedFields?size > 0]
        # injected fields
[#list injectedFields as fieldName]
        '${fieldName}',
[/#list]
[/#if]
    )

    def __init__(self, input_source, lex_state=LexicalState.${lexerData.lexicalStates[0].name}, line=1, column=1):
${grammar.utils.translateLexerInjections(injector, true)}
        if not input_source:
            raise ValueError('input filename not specified')
        self.input_source = input_source
        text = _input_text(input_source)
        self.content = self.munge_content(text, ${grammar.tabsToSpaces}, ${PRESERVE_LINE_ENDINGS}, ${JAVA_UNICODE_ESCAPE}, ${ENSURE_FINAL_EOL})
        self.content_len = n = len(self.content)
        n += 1
[#if grammar.lexerUsesParser]
        self.parser = None
[/#if]
        self._buffer_position = 0
        self._line_offsets = self.create_line_offsets_table(self.content)
        self._token_location_table = [None] * n
        self._token_offsets = BitSet(n)
        self._dummy_start_token = InvalidToken(self, 0, 0)
        self._ignored = InvalidToken(self, 0, 0)
        # The following two BitSets are used to store the current active
        # NFA states in the core tokenization loop
        self.next_states = BitSet(MAX_STATES)
        self.current_states = BitSet(MAX_STATES)

        self.active_token_types = set(TokenType)
  [#if grammar.deactivatedTokens?size>0]
       [#list grammar.deactivatedTokens as token]
        self.active_token_types.remove(TokenType.${token})
       [/#list]
  [/#if]
[#--
        # Holder for invalid characters, i.e. that cannot be matched as part of a token
        self.pending_invalid_chars = [] --]

        # Just used to "bookmark" the starting location for a token
        # for when we put in the location info at the end.
        self.starting_line = line
        self.starting_column = column

        # Token types that are "regular" tokens that participate in parsing,
        # i.e. declared as TOKEN
        [@EnumSet "regular_tokens" lexerData.regularTokens.tokenNames 8 /]
        # Token types that do not participate in parsing, a.k.a. "special" tokens in legacy JavaCC,
        # i.e. declared as UNPARSED (or SPECIAL_TOKEN)
        [@EnumSet "unparsed_tokens" lexerData.unparsedTokens.tokenNames 8 /]
        [#-- Tokens that are skipped, i.e. SKIP --]
        [#-- @EnumSet "skipped_tokens" lexerData.skippedTokens.tokenNames 8 / --]
        # Tokens that correspond to a MORE, i.e. that are pending
        # additional input
        [@EnumSet "more_tokens" lexerData.moreTokens.tokenNames 8 /]
        self.invalid_token = None
        self.previous_token = None
        self.lexical_state = None
        self.switch_to(lex_state)

    #
    # An internal method for getting the next token.
    # Most of the work is done in the private method
    # _next_token, which invokes the NFA machinery
    #
    def _get_next_token(self):
        while True:
            token = self._next_token()
            if not isinstance(token, InvalidToken):
                break

        if self.invalid_token:
            self.invalid_token.token_source = self
            it = self.invalid_token
            self.invalid_token = None
    [#if grammar.faultTolerant]
            it.is_unparsed = True
    [/#if]
            self.cache_token(it)
            return it
        self.cache_token(token)
        return token

    #
    # The public method for getting the next token.
    # If the tok parameter is None, it just tokenizes
    # starting at the internal buffer_position;
    # otherwise, it checks if we have already cached
    # the token after this one. If not, it finally
    # goes to the NFA machinery
    #

    def get_next_token(self, tok=None):
        if tok is None:
            return self._get_next_token()
        cached_token = tok.next_cached_token
        # If not currently active, discard it
        if cached_token and cached_token.type not in self.active_token_types:
            self.reset(tok)
            cached_token = None
        if cached_token:
            return cached_token
        return self.get_next_token_at_offset(tok.end_offset)

    def get_next_token_at_offset(self, offset):
        self.go_to(offset)
        return self.get_next_token(None)

    def read_char(self):
        bp = self._buffer_position
        cl = self.content_len
        while self._token_location_table[bp] == self._ignored and bp < cl:
            bp += 1
        if bp >= cl:
            self._buffer_position = bp
            return ''
        ch = self.content[bp]
        self._buffer_position = bp + 1
        return ch

    # The main method to invoke the NFA machinery
    def _next_token(self):
        matched_token = None
        in_more = False
        token_begin_offset = self._buffer_position
        # first_char = ''
        # The core tokenization loop
        read_char = self.read_char
        # get_line_from_offset = self.get_line_from_offset
        # get_codepoint_column_from_offset = self.get_codepoint_column_from_offset
        while matched_token is None:
            matched_type = None
            matched_pos = code_units_read = 0
            reached_end = False
            if in_more:
                cur_char = read_char()
                if not cur_char:
                    reached_end = True
            else:
                token_begin_offset = self._buffer_position
                # first_char = cur_char = read_char()
                cur_char = read_char()
                if cur_char == '':
                    matched_type = TokenType.EOF
                    reached_end = True

[#if multipleLexicalStates]
            # Get the NFA function table current lexical state
            # There is some possibility that there was a lexical state change
            # since the last iteration of this loop!
[/#if]
            nfa_functions = get_function_table_map(self.lexical_state)
            # the core NFA loop
            if not reached_end:
                while True:
                    # Holder for the new type (if any) matched on this iteration
                    new_type = None
                    if code_units_read > 0:
                        # What was next_states on the last iteration
                        # is now the current_states!
                        temp = self.current_states
                        self.current_states = self.next_states
                        self.next_states = temp
                        retval = read_char()
                        if retval:
                            cur_char = retval
                        else:
                            reached_end = True
                            break
                    self.next_states.clear()
                    if code_units_read == 0:
                        returned_type = nfa_functions[0](cur_char, self.next_states, self.active_token_types)
                        if returned_type and (new_type is None or returned_type.value < new_type.value):
                            new_type = returned_type
                    else:
                        next_active = self.current_states.next_set_bit(0)
                        while next_active != -1:
                            returned_type = nfa_functions[next_active](cur_char, self.next_states, self.active_token_types)
                            if returned_type and (new_type is None or returned_type.value < new_type.value):
                                new_type = returned_type
                            next_active = self.current_states.next_set_bit(next_active + 1)
                    code_units_read += 1
                    if new_type:
                        matched_type = new_type
                        in_more = matched_type in self.more_tokens
                        matched_pos = code_units_read
                    if self.next_states.is_empty:
                        break
            if matched_type is None:
                self._buffer_position = token_begin_offset + 1
                return InvalidToken(self, token_begin_offset, self._buffer_position)
            self._buffer_position -= code_units_read - matched_pos
            if matched_type in self.regular_tokens or matched_type in self.unparsed_tokens:
                # import pdb; pdb.set_trace()
                matched_token = new_token(matched_type, self, token_begin_offset, self._buffer_position)
                matched_token.is_unparsed = matched_type not in self.regular_tokens
[#list grammar.lexerTokenHooks as tokenHookMethodName]
  [#if tokenHookMethodName = "CommonTokenAction"]
                self.${tokenHookMethodName}(matched_token)
  [#else]
                matched_token = self.${tokenHookMethodName}(matched_token)
  [/#if]
[/#list]
[#if lexerData.hasTokenActions]
            matched_token = self.token_lexical_actions(matched_token, matched_type)
[/#if]
[#if multipleLexicalStates]
            self.do_lexical_state_switch(matched_type)
[/#if]
        return matched_token

[#if multipleLexicalStates]
    def do_lexical_state_switch(self, token_type):
        new_state = token_type_to_lexical_state_map.get(token_type)
        if new_state is None:
            return False
        return self.switch_to(new_state)

[/#if]

    #
    # Switch to specified lexical state.
    #
    def switch_to(self, lex_state):
        if self.lexical_state != lex_state:
            self.lexical_state = lex_state
            return True
        return False

    def go_to(self, offset):
        tlt = self._token_location_table
        while tlt[offset] is self._ignored and offset < self.content_len:
            offset += 1
        self._buffer_position = offset

    # Reset the token source input
    # to just after the Token passed in.
    def reset(self, t, lex_state=None):
[#list grammar.resetTokenHooks as resetTokenHookMethodName]
        self.${grammar.utils.translateIdentifier(resetTokenHookMethodName)}(t)
[/#list]
        self.go_to(t.end_offset)
        self.uncache_tokens(t)
        if lex_state:
            self.switch_to(lex_state)
[#if multipleLexicalStates]
        else:
            self.do_lexical_state_switch(t.type)
[/#if]

 [#if lexerData.hasTokenActions]
    def token_lexical_actions(self, matched_token, matched_type):
    [#var idx = 0]
    [#list lexerData.regularExpressions as regexp]
        [#if regexp.codeSnippet?has_content]
        [#if idx > 0]el[/#if]if matched_type == TokenType.${regexp.label}:
${grammar.utils.translateCodeBlock(regexp.codeSnippet.javaCode, 12)}
          [#set idx = idx + 1]
        [/#if]
    [/#list]
        return matched_token
 [/#if]

    def munge_content(self, content, tabs_to_spaces, preserve_lines,
                      java_unicode_escape, ensure_final_endline):
        if tabs_to_spaces <= 0 and preserve_lines and not java_unicode_escape:
            if ensure_final_endline:
                last_char = content[-1]
                if last_char != '\n' and last_char != '\r':
                    content += '\n'
            return content

        buf = []
        index = 0
        # This is just to handle tabs to spaces. If you don't have that setting set, it
        # is really unused.
        col = 0
        # just_saw_unicode_escape = False
        # Don't know if this is really needed for Python ...
        code_points = list(content)
        cplen = len(code_points)
        while index < cplen:
            ch = code_points[index]
            index += 1
            if ch == '\\' and java_unicode_escape and index < cplen:
                ch = code_points[index]
                index += 1
                if ch != 'u':
                    # just_saw_unicode_escape = False
                    buf.append('\\')
                    buf.append(ch)
                    if ch == '\n':
                        col = 0
                    else:
                        col += 2
                else:
                    while code_points[index] == 'u':
                        index += 1
                        # col += 1
                    hex_buf = []
                    for i in range(4):
                        hex_buf.append(code_points[index])
                        index += 1
                    current = int(''.join(hex_buf), 16)
                    # last = buf[-1] if len(buf) > 0 else ''
                    # shouldn't see surrogate pairs, normally
                    buf.append(chr(current))
                    # just_saw_unicode_escape = True
                    # col += 6
                    col += 1
                    # We're not going to be trying to track line/column information relative to the original content
                    # with tabs or unicode escape, so we just increment 1, not 6
            elif ch == '\r' and not preserve_lines:
                # just_saw_unicode_escape = False
                buf.append('\n')
                if index < cplen:
                    ch = code_points[index]
                    index += 1
                    if ch != '\n':
                        buf.append(ch)
                        col += 1
                    else:
                        col = 0
            elif ch == '\t' and tabs_to_spaces > 0:
                # just_saw_unicode_escape = False
                spaces_to_add = tabs_to_spaces - col % tabs_to_spaces
                for i in range(spaces_to_add):
                    buf.append(' ')
                    col += 1
            else:
                # just_saw_unicode_escape = False
                buf.append(ch)
                if ch == '\n':
                    col = 0
                else:
                    col += 1
        if ensure_final_endline:
            last_char = buf[-1] if len(buf) > 0 else ''
            if last_char != '\n' and last_char != '\r':
                buf.append('\n')
        return ''.join(buf)

    def create_line_offsets_table(self, content):
        if not content:
            return [0]
        length = len(content)
        line_count = content.count('\n')
        if content[-1] != '\n':
            line_count += 1
        result = [0]
        for i in range(length):
            ch = content[i]
            if ch == '\n':
                if (i + 1) == length:
                    break
                result.append(i + 1)
        return result

    def get_line_from_offset(self, pos):
        if pos >= self.content_len:
            result = len(self._line_offsets)
            if self.content[-1] != '\n':
                result -= 1
        else:
            sr = bisect.bisect_right(self._line_offsets, pos) - 1
            if sr >= 0:
                result = sr
            else:
                # import pdb; pdb.set_trace()
                result = sr + 1
        return self.starting_line + result

    def get_codepoint_column_from_offset(self, pos):
        if pos >= self.content_len:
            return 1
        if pos == 0:
            return self.starting_column
        # import pdb; pdb.set_trace()
        line = self.get_line_from_offset(pos) - self.starting_line
        line_start = self._line_offsets[line]
        adjustment = 1 if line > 0 else self.starting_column
        return adjustment + pos - line_start

    def cache_token(self, tok):
[#if !grammar.minimalToken]
        if tok.is_inserted:
            next = tok.next_cached_token
            if next:
                self.cache_token(next)
            return
[/#if]
        offset = tok.begin_offset
        self._token_offsets.set(offset)
        self._token_location_table[offset] = tok

    def uncache_tokens(self, last_token):
        end_offset = last_token.end_offset
        if end_offset < self._token_offsets.bits:
            self._token_offsets.clear(end_offset, self._token_offsets.bits)
[#if !grammar.minimalToken]
        last_token.unset_appended_token()
[/#if]

    def next_cached_token(self, offset):
        next_offset = self._token_offsets.next_set_bit(offset)
        return self._token_location_table[next_offset] if next_offset >= 0 else None

    def previous_cached_token(self, offset):
        prev_offset = self._token_offsets.previous_set_bit(offset - 1)
        return self._token_location_table[prev_offset] if prev_offset >= 0 else None

    def get_text(self, start_offset, end_offset):
        chars = []
        tlt = self._token_location_table
        content = self.content
        for offset in range(start_offset, end_offset):
            if tlt[offset] is not self._ignored:
                chars.append(content[offset])
        return ''.join(chars)

${grammar.utils.translateLexerInjections(injector, false)}

 [#if lexerData.hasLexicalStateTransitions]
# Generate the map for lexical state transitions from the various token types (if necessary)
    [#list grammar.lexerData.regularExpressions as regexp]
      [#if !regexp.newLexicalState?is_null]
token_type_to_lexical_state_map[TokenType.${regexp.label}] = LexicalState.${regexp.newLexicalState.name}
      [/#if]
    [/#list]
 [/#if]
