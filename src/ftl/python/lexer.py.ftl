[#ftl strict_vars=true]
# Copyright (C) 2021 Vinay Sajip, vinay_sajip@yahoo.co.uk
# Parser lexing package. Generated by ${generated_by}. Do not edit.

import bisect
import logging

from .tokens import TokenType, LexicalState, Token, InvalidToken, new_token
from .utils import BitSet, FileLineMap, as_chr, List

[#var NFA_RANGE_THRESHOLD = 16]
[#var MAX_INT=2147483647]
[#var lexerData=grammar.lexerData]
[#var multipleLexicalStates = lexerData.lexicalStates.size() > 1]
[#var TT = "TokenType."]

logger = logging.getLogger(__name__)

#
# Hack to allow token types to be referenced in snippets without
# qualifying
#
globals().update(TokenType.__members__)

# NFA code and data
[#if multipleLexicalStates]
# A mapping from lexical state to NFA functions for that state.
[#-- We only need the mapping if there is more than one lexical state.--]
function_table_map = {}
[/#if]

[#list lexerData.lexicalStates as lexicalState]
[@GenerateStateCode lexicalState/]
[/#list]

# Just use binary search to check whether the char is in one of the
# intervals
def check_intervals(ranges, ch):
    index = bisect.bisect_left(ranges, ch)
    if index < len(ranges) and ranges[index] == ch:
        return True
    return False

[#--
  Generate all the NFA transition code
  for the given lexical state
--]
[#macro GenerateStateCode lexicalState]
[#list lexicalState.allNfaStates as nfaState]
[#if nfaState.moveRanges.size() >= NFA_RANGE_THRESHOLD]
[@GenerateMoveArray nfaState/]
[/#if]
[@GenerateNfaStateMethod nfaState/]
[/#list]

def NFA_FUNCTIONS_${lexicalState.name}_init():
[#--
    In theory this could be initialized as a normal list, but it's
    not clear if state.index is always the same as state_index

    Update: it is, see https://github.com/javacc21/javacc21/issues/72

    functions = [None] * ${lexicalState.allNfaStates.size()}
    [#list lexicalState.allNfaStates as state]
    functions[${state.index}] = ${state.methodName}
    [/#list]
--]
    functions = [
[#list lexicalState.allNfaStates as state]
        ${state.methodName}[#if state_has_next],[/#if]
[/#list]
    ]
[#if multipleLexicalStates]
    function_table_map[LexicalState.${lexicalState.name}] = functions
[#else]
    return functions
[/#if]

[#if multipleLexicalStates]
NFA_FUNCTIONS_${lexicalState.name}_init()
[#else]
nfa_functions = NFA_FUNCTIONS_${lexicalState.name}_init()
[/#if]

def get_function_table_map(lexical_state):
    [#if multipleLexicalStates]
    return function_table_map[lexical_state]
    [#else]
    # We only have one lexical state in this case, so we return that!
    return nfa_functions
    [/#if]

[/#macro]

[#--
   Generate the array representing the characters
   that this NfaState "accepts".
   This corresponds to the moveRanges field in
   com.javacc.core.NfaState
--]
[#macro GenerateMoveArray nfaState]
[#var moveRanges = nfaState.moveRanges]
[#var arrayName = nfaState.movesArrayName]
[#-- No need to create an array and populate one by one - just
     initialize normally

def ${arrayName}_init():
     it!
    result = [0] * ${nfaState.moveRanges.size()}
[#list nfaState.moveRanges as char]
    result[${char_index}] = ${grammar.utils.displayChar(char)}
    return result
${arrayName} = ${arrayName}_init()
[/#list--]
${arrayName} = [
[#list nfaState.moveRanges as char]
    as_chr(${grammar.utils.displayChar(char)})[#if char_has_next],[/#if]
[/#list]
]
[/#macro]

[#--
   Generate the method that represents the transition
   (or transitions if this is a CompositeStateSet)
   that correspond to an instanceof com.javacc.core.NfaState
--]
[#macro GenerateNfaStateMethod nfaState]
  [#if !nfaState.composite]
     [@GenerateSimpleNfaMethod nfaState/]
  [#else]
def ${nfaState.methodName}(ch, next_states):
    [#var states = nfaState.orderedStates]
    [#-- sometimes set in the code below --]
    type = None
    [#var useElif = false]
    [#list states as state]
      [#var isFirstOfGroup=true, isLastOfGroup=true, jumpOut = !state_has_next]
      [#if state_index!=0]
         [#set isFirstOfGroup = !states[state_index-1].moveRanges.equals(state.moveRanges)]
      [/#if]
      [#if state_has_next]
         [#set isLastOfGroup = !states[state_index+1].moveRanges.equals(state.moveRanges)]
         [#if isLastOfGroup]
            [#set jumpOut = state.isNonOverlapping(states.subList(state_index+1, states?size))]
         [/#if]
      [/#if]
      [@GenerateStateMove state isFirstOfGroup isLastOfGroup jumpOut useElif /]
      [#if state_has_next && !jumpOut && isLastOfGroup && states[state_index+1].isNonOverlapping(states.subList(0, state_index+1))]
        [#set useElif = true]
      [#else]
        [#set useElif = false]
      [/#if]
    [/#list]
    return type

  [/#if]
[/#macro]

[#--
  Generates the code for an NFA state transition
  within a composite state. This code is a bit tricky
  because it consolidates more than one condition in
  a single conditional block. The jumpOut parameter says
  whether we can just jump out of the method.
  (This is based on whether any of the moveRanges
  for later states overlap. If not, we can jump out.)
--]
[#macro GenerateStateMove nfaState isFirstOfGroup isLastOfGroup jumpOut useElif=false]
  [#var nextState = nfaState.nextState.canonicalState]
  [#var type = nfaState.nextState.type]
    [#if isFirstOfGroup]
    [#if useElif]elif[#else]if[/#if] [@NfaStateCondition nfaState /]:
    [/#if]
      [#if nextState.index >= 0]
        next_states.set(${nextState.index})
      [/#if]
   [#if isLastOfGroup]
      [#if jumpOut]
        return [#if type??]${TT}${type.label}[#else]None[/#if]
      [#elseif type??]
        type = ${TT}${type.label}
     [/#if]
   [/#if]
[/#macro]

[#--
  Generate the code for a simple (non-composite) NFA state
--]
[#macro GenerateSimpleNfaMethod nfaState]
def ${nfaState.methodName}(ch, next_states):
[#var nextState = nfaState.nextState.canonicalState]
[#var type = nfaState.nextState.type]
    if [@NfaStateCondition nfaState /]:
        [#if nextState.index >= 0]
        next_states.set(${nextState.index})
        [/#if]
      [#if type??]
        return ${TT}${type.label}
      [/#if]
    [#--return None--]

[/#macro]

[#--
Generate the condition part of the NFA state transition
If the size of the moveRanges vector is greater than NFA_RANGE_THRESHOLD
it uses the canned binary search routine. For the smaller moveRanges
it just generates the inline conditional expression
--]
[#macro NfaStateCondition nfaState]
    [#if nfaState.moveRanges?size < NFA_RANGE_THRESHOLD]
      [@RangesCondition nfaState.moveRanges /][#t]
    [#elseif nfaState.hasAsciiMoves && nfaState.hasNonAsciiMoves]
      ([@RangesCondition nfaState.asciiMoveRanges/]) or (ch >= chr(128) and check_intervals(${nfaState.movesArrayName}, ch))[#t]
    [#else]
      check_intervals(${nfaState.movesArrayName}, ch)[#t]
    [/#if]
[/#macro]

[#--
This is a recursive macro that generates the code corresponding
to the accepting condition for an NFA state. It is used
if NFA state's moveRanges array is smaller than NFA_RANGE_THRESHOLD
(which is set to 16 for now)
--]
[#macro RangesCondition moveRanges]
    [#var left = moveRanges[0], right = moveRanges[1]]
    [#var displayLeft = grammar.utils.displayChar(left), displayRight = grammar.utils.displayChar(right)]
    [#var singleChar = left == right]
    [#if moveRanges?size==2]
       [#if singleChar]
          ch == as_chr(${displayLeft})[#t]
       [#elseif left +1 == right]
          ch == as_chr(${displayLeft}) or ch == as_chr(${displayRight})[#t]
       [#elseif left > 0]
          ch >= as_chr(${displayLeft})[#t]
          [#if right < 1114111]
 and ch <= as_chr(${displayRight}) [#rt]
          [/#if]
       [#else]
           ch <= as_chr(${displayRight}) [#t]
       [/#if]
    [#else]
       ([@RangesCondition moveRanges[0..1]/]) or ([@RangesCondition moveRanges[2..moveRanges?size-1]/])[#t]
    [/#if]
[/#macro]

# Lexer code and data

[#var tokenBuilderClass = grammar.hugeFileSupport?string("TokenBuilder", "FileLineMap")]

[#macro EnumSet varName tokenNames indent=0]
[#var indstr = ""]
[#if indent > 0]
[#set indstr = indstr?right_pad(indent)]
[/#if]
   [#if tokenNames?size=0]
${indstr}self.${varName} = set()
   [#else]
${indstr}self.${varName} = {
     [#list tokenNames as type]
${indstr}    TokenType.${type}[#if type_index < (tokenNames?size - 1)],[/#if]
       [/#list]
${indstr}}
   [/#if]
[/#macro]

[#list grammar.parserCodeImports as import]
   ${import}
[/#list]

[#if lexerData.hasLexicalStateTransitions]
# A mapping for lexical state transitions triggered by a certain token type (token type -> lexical state)
token_type_to_lexical_state_map = {}
[/#if]
[#var injector = grammar.injector]

[#-- #var lexerClassName = grammar.lexerClassName --]
[#var lexerClassName = "Lexer"]
class ${lexerClassName}:

    def __init__(self, input_source, stream, lex_state=LexicalState.${lexerData.lexicalStates[0].name}, line=1, column=1):
${grammar.utils.translateLexerInjections(injector, true)}
        self.input_source = input_source
[#if grammar.lexerUsesParser]
        self.parser = None
[/#if]
        # The following two BitSets are used to store the current active
        # NFA states in the core tokenization loop
        self.next_states = BitSet()
        self.current_states = BitSet()

        # Holder for the pending characters we read from the input stream
        self._char_buf = []

        self.active_token_types = set(TokenType)
  [#if grammar.deactivatedTokens?size>0]
       [#list grammar.deactivatedTokens as token]
        self.active_token_types.remove(TokenType.${token})
       [/#list]
  [/#if]
[#--
        # Holder for invalid characters, i.e. that cannot be matched as part of a token
        self.pending_invalid_chars = [] --]

        # Just used to "bookmark" the starting location for a token
        # for when we put in the location info at the end.
        self.token_begin_line = -1
        self.token_begin_column = -1

        # Token types that are "regular" tokens that participate in parsing,
        # i.e. declared as TOKEN
        [@EnumSet "regular_tokens" lexerData.regularTokens.tokenNames 8 /]
        # Token types that do not participate in parsing, a.k.a. "special" tokens in legacy JavaCC,
        # i.e. declared as UNPARSED (or SPECIAL_TOKEN)
        [@EnumSet "unparsed_tokens" lexerData.unparsedTokens.tokenNames 8 /]
        # Tokens that are skipped, i.e. SKIP
        [@EnumSet "skipped_tokens" lexerData.skippedTokens.tokenNames 8 /]
        # Tokens that correspond to a MORE, i.e. that are pending
        # additional input
        [@EnumSet "more_tokens" lexerData.moreTokens.tokenNames 8 /]
        self.trace_enabled = [#if grammar.debugLexer]True[#else]False[/#if]
        self.invalid_token = None
        self.previous_token = None
        self.lexical_state = None
        self.input_stream = FileLineMap(input_source, stream, line, column)
        self.switch_to(lex_state)

    #
    # The public method for getting the next token.
    # Most of the work is done in the private method
    # _next_token, which invokes the NFA machinery
    #
    def get_next_token(self):
        while True:
            token = self._next_token()
            if not isinstance(token, InvalidToken):
                break

        if self.invalid_token:
            self.invalid_token.next_token = token
            token.previous_token = self.invalid_token
            it = self.invalid_token
            self.invalid_token = None
    [#if grammar.faultTolerant]
            it.unparsed = True
    [/#if]
            return it
        token.previous_token = self.previous_token
        if self.previous_token:
            self.previous_token.next_token = token
        self.previous_token = token
        return token

    # The main method to invoke the NFA machinery
    def _next_token(self):
        matched_token = None
        in_more = False
        # The core tokenization loop
        input_stream = self.input_stream
        while matched_token is None:
            matched_type = None
            matched_pos = chars_read = 0
            if in_more:
                cur_char = input_stream.read_char()
                if cur_char:
                    self._char_buf.append(cur_char)
            else:
                self._char_buf = []
[#-- The following is a temporary kludge. Need to rewrite the LegacyTokenBuilder
     that the hugeFileSupport option uses.
     It's surely broken in various ways, like wrt full Unicode etc.--]
[#if grammar.hugeFileSupport]
                cur_char = input_stream.begin_token()
                self.token_begin_line = input_stream.get_begin_line()
                self.token_begin_column = input_stream.get_begin_column()
[#else]
                self.token_begin_line = input_stream.get_line()
                self.token_begin_column = input_stream.get_column()
                cur_char = input_stream.read_char()
[/#if]
                if self.trace_enabled:
                    logger.info('Starting new token on line: %d, column: %d' % (self.token_begin_line, self.token_begin_column))
                if cur_char == '':
                    if self.trace_enabled:
                        logger.info('Reached end of input')
                    matched_type = TokenType.EOF
                else:
                    if self.trace_enabled:
                        logger.info('Read character %r', cur_char)
                    self._char_buf.append(cur_char)

[#if multipleLexicalStates]
           # Get the NFA function table current lexical state
           # There is some possibility that there was a lexical state change
           # since the last iteration of this loop!
[/#if]
            nfa_functions = get_function_table_map(self.lexical_state)
            # the core NFA loop
            if matched_type != TokenType.EOF:
                while True:
                    # Holder for the new type (if any) matched on this iteration
                    new_type = None
                    if chars_read > 0:
                        # What was next_states on the last iteration
                        # is now the current_states!
                        temp = self.current_states
                        self.current_states = self.next_states
                        self.next_states = temp
                        retval = input_stream.read_char()
                        if self.trace_enabled:
                            logger.info('Read character %r', retval)
                        if retval:
                            cur_char = retval
                            self._char_buf.append(cur_char)
                        else:
                            break
                        self.next_states.clear()
                    if chars_read == 0:
                        new_type = nfa_functions[0](cur_char, self.next_states)
                        if new_type not in self.active_token_types:
                            new_type = None
                    else:
                        next_active = self.current_states.next_set_bit(0)
                        while next_active != -1:
                            returned_type = nfa_functions[next_active](cur_char, self.next_states)
                            if returned_type in self.active_token_types and (new_type is None or returned_type.value < new_type.value):
                                new_type = returned_type
                                if self.trace_enabled:
                                    logger.info('Potential match: %s', new_type)
                            next_active = self.current_states.next_set_bit(next_active + 1)
                    chars_read += 1
                    if new_type:
                        matched_type = new_type
                        in_more = matched_type in self.more_tokens
                        matched_pos = chars_read
                    if self.next_states.is_empty:
                        break
            if matched_type is None:
                self.backup(chars_read - 1)
                if self.trace_enabled:
                   logger.info('Invalid input: %r', self._char_buf[0])
                return self.handle_invalid_char(self._char_buf[0])
            else:
              if self.trace_enabled:
                  logger.info('Matched pattern of type: %s: %r', matched_type, self._char_buf)

            if chars_read > matched_pos:
                self.backup(chars_read - matched_pos)
            if matched_type in self.regular_tokens or matched_type in self.unparsed_tokens:
                matched_token = self.instantiate_token(matched_type)
            [#if lexerData.hasTokenActions]
            matched_token = self.token_lexical_actions(matched_token, matched_type)
            [/#if]
            [#if multipleLexicalStates]
            self.do_lexical_state_switch(matched_type)
            [/#if]
        return matched_token

    def backup(self, amount):
        self.input_stream.backup(amount)
        if amount:
            self._char_buf = self._char_buf[:-amount]

[#if multipleLexicalStates]
    def do_lexical_state_switch(self, token_type):
        new_state = token_type_to_lexical_state_map.get(token_type)
        if new_state is None:
            return False
        return self.switch_to(new_state)

[/#if]

    #
    # Switch to specified lexical state.
    #
    def switch_to(self, lex_state):
        if self.lexical_state != lex_state:
            if self.trace_enabled:
                logger.info('Switching from lexical state %s to %s',
                            self.lexical_state, lex_state)
            self.lexical_state = lex_state
            return True
        return False

 [#if grammar.hugeFileSupport]
    [#embed "LegacyTokenBuilder.java.ftl"]
 [#else]
    # Reset the token source input
    # to just after the Token passed in.
    def reset(self, t, lex_state=None):
        self.input_stream.go_to(t.end_line, t.end_column)
        self.input_stream.forward(1)
        t.set_next(None)
        t.next = None
        if lex_state:
            seld.switch_to(lex_state)

 [/#if]

    def handle_invalid_char(self, ch):
        line = self.input_stream.end_line
        column = self.input_stream.end_column
        img = ch
        if self.invalid_token is None:
            self.invalid_token = it = InvalidToken(img, self.input_source)
            it.begin_line = line
            it.begin_column = column
        else:
            it = self.invalid_token
            it.image +=  img
        it.end_line = line
        it.end_column = column
        return it

    def instantiate_token(self, type):
        tokenImage = ''.join(self._char_buf)
[#if grammar.settings.TOKEN_FACTORY??]
        matched_token = ${grammar.settings.TOKEN_FACTORY}.new_token(type, tokenImage, self.input_source)
[#elseif !grammar.hugeFileSupport]
        matched_token = new_token(type, tokenImage, self)
[#else]
        matched_token = new_token(type, tokenImage, self.input_source)
[/#if]
        matched_token.begin_line = self.token_begin_line
        matched_token.begin_column = self.token_begin_column
        matched_token.end_line = self.input_stream.end_line
        matched_token.end_column = self.input_stream.end_column
        matched_token.input_source = self.input_source
        if self.previous_token is not None:
            matched_token.previous_token = self.previous_token
            self.previous_token.next_token = matched_token
        matched_token.unparsed = type in self.unparsed_tokens
 [#list grammar.lexerTokenHooks as tokenHookMethodName]
    [#if tokenHookMethodName = "CommonTokenAction"]
        self.${tokenHookMethodName}(matched_token)
    [#else]
        matched_token = self.${tokenHookMethodName}(matched_token)
    [/#if]
 [/#list]
        return matched_token

 [#if lexerData.hasTokenActions]
    def token_lexical_actions(self, matched_token, matched_type):
        # raise NotImplementedError('Token lexical actions are present, but not implemented - awaiting support for language-specific snippets')
[#--
    switch(matchedType) {
   [#list lexerData.regularExpressions as regexp]
        [#if regexp.codeSnippet?has_content]
		  case ${regexp.label} :
		      ${grammar.utils.regexp.codeSnippet.javaCode}
           break;
        [/#if]
   [/#list]
      default : break;
    }
  }
--]
    [#var idx = 0]
    [#list lexerData.regularExpressions as regexp]
        [#if regexp.codeSnippet?has_content]
        [#if idx > 0]el[/#if]if matched_type == TokenType.${regexp.label}:
${grammar.utils.translateCodeBlock(regexp.codeSnippet.javaCode, 12)}
          [#set idx = idx + 1]
        [/#if]
    [/#list]
        return matched_token
 [/#if]

${grammar.utils.translateLexerInjections(injector, false)}

 [#if lexerData.hasLexicalStateTransitions]
# Generate the map for lexical state transitions from the various token types (if necessary)
    [#list grammar.lexerData.regularExpressions as regexp]
      [#if !regexp.newLexicalState?is_null]
token_type_to_lexical_state_map[TokenType.${regexp.label}] = LexicalState.${regexp.newLexicalState.name}
      [/#if]
    [/#list]
 [/#if]
